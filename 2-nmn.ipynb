{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2124915",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85a8ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f791f",
   "metadata": {},
   "source": [
    "### Load CLEVR data & Define functions to build vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2a352a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of answers: 28\n",
      "Num attributes: 15\n"
     ]
    }
   ],
   "source": [
    "def build_answer_vocab(questions):\n",
    "    answers = sorted(set(q[\"answer\"] for q in questions))\n",
    "    return {a: i for i, a in enumerate(answers)}\n",
    "\n",
    "def build_attr_vocab(questions):\n",
    "    attrs = set()\n",
    "    for q in questions:\n",
    "        for step in q[\"program\"]:\n",
    "            if step[\"function\"].startswith(\"filter_\"):\n",
    "                attrs.add(step[\"value_inputs\"][0])\n",
    "    return {a:i for i,a in enumerate(sorted(attrs))}\n",
    "\n",
    "with open(\"data/CLEVR_v1.0/questions/CLEVR_train_questions.json\") as f:\n",
    "    questions_json = json.load(f)[\"questions\"]\n",
    "\n",
    "ANSWER_VOCAB = build_answer_vocab(questions_json)\n",
    "IDX_TO_ANSWER = {v: k for k, v in ANSWER_VOCAB.items()}\n",
    "NUM_ANSWERS = len(ANSWER_VOCAB)\n",
    "\n",
    "ATTR_VOCAB = build_attr_vocab(questions_json)\n",
    "ATTR_VOCAB_SIZE = len(ATTR_VOCAB)\n",
    "\n",
    "REL_VOCAB = {\"left\":0, \"right\":1, \"front\":2, \"behind\":3}\n",
    "REL_VOCAB_SIZE = len(REL_VOCAB)\n",
    "\n",
    "print(\"Number of answers:\", NUM_ANSWERS)\n",
    "print(\"Num attributes:\", ATTR_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1a252",
   "metadata": {},
   "source": [
    "### CLEVR -> NMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "162d4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clevr_to_nmn(program, attr_vocab, rel_vocab=None):\n",
    "    # If already NMN-style tuples, return unchanged\n",
    "    if len(program) > 0 and isinstance(program[0], tuple):\n",
    "        return program\n",
    "\n",
    "    nmn_program = []\n",
    "    for step in program:\n",
    "        f = step[\"function\"]\n",
    "\n",
    "        if f == \"scene\":\n",
    "            nmn_program.append((\"scene\", None))\n",
    "\n",
    "        elif f.startswith(\"filter_\"):\n",
    "            attr = step[\"value_inputs\"][0]\n",
    "            nmn_program.append((\"attend\", attr_vocab[attr]))\n",
    "\n",
    "        elif f.startswith(\"relate_\"):\n",
    "            # optional: if you want relation types\n",
    "            rel = f.split(\"_\")[1]\n",
    "            if rel_vocab:\n",
    "                nmn_program.append((\"relate\", rel_vocab[rel]))\n",
    "            else:\n",
    "                nmn_program.append((\"relate\", None))\n",
    "        \n",
    "        elif f in (\"intersect\", \"union\"):\n",
    "            nmn_program.append((\"combine\", None))\n",
    "\n",
    "        elif f == \"unique\":\n",
    "            nmn_program.append((\"unique\", None))\n",
    "\n",
    "        elif f == \"count\":\n",
    "            nmn_program.append((\"measure\", \"count\"))\n",
    "        elif f == \"exist\":\n",
    "            nmn_program.append((\"measure\", \"exist\"))\n",
    "\n",
    "        elif f == \"query\" or f.startswith(\"query\"):\n",
    "            # simplify to a measure (only for demo)\n",
    "            nmn_program.append((\"measure\", \"query\"))\n",
    "        \n",
    "        elif f == \"same\" or f.startswith(\"same_\"):\n",
    "            # simplify to a measure (only for demo)\n",
    "            nmn_program.append((\"measure\", \"same\"))\n",
    "\n",
    "        # unknown function\n",
    "        else:\n",
    "            # treat unknown as attend\n",
    "            nmn_program.append((\"attend\", 0))\n",
    "\n",
    "    return nmn_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "516587c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_program(program):\n",
    "    normalized = []\n",
    "    for op, arg in program:\n",
    "        if op == \"classify\":\n",
    "            normalized.append((\"measure\", arg))\n",
    "        else:\n",
    "            normalized.append((op, arg))\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a8a39",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "90d378fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone = models.resnet18(pretrained=True)\n",
    "backbone = nn.Sequential(*list(backbone.children())[:-2])\n",
    "backbone = backbone.to(device)\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "backbone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae304db7",
   "metadata": {},
   "source": [
    "### Define Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ae8de14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttendModule(nn.Module):\n",
    "    def __init__(self, feat_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, feat_dim)\n",
    "        self.conv = nn.Conv2d(feat_dim, feat_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, features, attr_idx):\n",
    "        w = self.embed(attr_idx).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = features * w\n",
    "        attn = self.conv(x)\n",
    "        return torch.sigmoid(attn)\n",
    "\n",
    "class RelateModule(nn.Module):\n",
    "    def __init__(self, feat_dim, rel_vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(rel_vocab_size, feat_dim)\n",
    "        self.conv = nn.Conv2d(feat_dim, feat_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, attn, rel_idx):\n",
    "        r = self.embed(rel_idx).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = attn * r\n",
    "        return self.conv(x)\n",
    "\n",
    "class CombineModule(nn.Module):\n",
    "    def forward(self, a1, a2):\n",
    "        return a1 * a2\n",
    "\n",
    "class ClassifyModule(nn.Module):\n",
    "    def __init__(self, feat_dim, num_answers):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feat_dim, num_answers)\n",
    "\n",
    "    def forward(self, attn, features):\n",
    "        weighted = (attn * features).sum(dim=[2, 3])\n",
    "        return self.fc(weighted)\n",
    "    \n",
    "class UniqueModule(nn.Module):\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(feat_dim, feat_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, attn):\n",
    "        return self.conv(attn)\n",
    "\n",
    "class MeasureModule(nn.Module):\n",
    "    def __init__(self, num_answers, feat_dim=512):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feat_dim, num_answers)\n",
    "\n",
    "    def forward(self, attn, op):\n",
    "        pooled = attn.mean(dim=(2,3))\n",
    "        return self.fc(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f89e8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMNComposer(nn.Module):\n",
    "    def __init__(self, module_dict):\n",
    "        super().__init__()\n",
    "        self.module_dict = nn.ModuleDict(module_dict)\n",
    "\n",
    "    def forward(self, features, program):\n",
    "        B, C, H, W = features.shape\n",
    "        stack = []\n",
    "\n",
    "        for op, arg in program:\n",
    "            if op == \"scene\":\n",
    "                stack = []\n",
    "                stack.append(torch.ones((B,1,H,W), device=features.device))\n",
    "\n",
    "            elif op == \"attend\":\n",
    "                stack.append(self.module_dict[\"attend\"](features, torch.tensor([arg]).to(features.device)))\n",
    "\n",
    "            elif op == \"relate\":\n",
    "                a = stack.pop()\n",
    "                stack.append(self.module_dict[\"relate\"](a, arg))\n",
    "\n",
    "            elif op == \"unique\":\n",
    "                a = stack.pop()\n",
    "                stack.append(self.module_dict[\"unique\"](a))\n",
    "\n",
    "            elif op == \"measure\":\n",
    "                a = stack.pop()\n",
    "                return self.module_dict[\"measure\"](a, features)\n",
    "\n",
    "        raise RuntimeError(\"Program ended without measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "782417d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRNMNDataset(Dataset):\n",
    "    def __init__(self, questions_json, image_dir, transform, answer_vocab):\n",
    "        # Load CLEVR questions\n",
    "        with open(questions_json) as f:\n",
    "            self.questions = json.load(f)[\"questions\"]\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.answer_vocab = answer_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q = self.questions[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(\n",
    "            self.image_dir,\n",
    "            f\"CLEVR_train_{q['image_index']:06d}.png\"\n",
    "        )\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Convert CLEVR program to NMN format\n",
    "        program = clevr_to_nmn(q[\"program\"], ATTR_VOCAB)\n",
    "\n",
    "        # Convert answer to integer\n",
    "        answer = self.answer_vocab[q[\"answer\"]]\n",
    "\n",
    "        return image, program, torch.tensor(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9659287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = torch.stack([b[0] for b in batch])\n",
    "    programs = [b[1] for b in batch]\n",
    "    answers = torch.tensor([b[2] for b in batch])\n",
    "    return images, programs, answers\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CLEVRNMNDataset(\n",
    "    questions_json=\"data/CLEVR_v1.0/questions/CLEVR_train_questions.json\",\n",
    "    image_dir=\"data/CLEVR_v1.0/images/train\",\n",
    "    transform=transform,\n",
    "    answer_vocab=ANSWER_VOCAB\n",
    ")\n",
    "\n",
    "subset = torch.utils.data.Subset(dataset, list(range(1000)))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    subset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "59f3e9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'inputs': [], 'function': 'scene', 'value_inputs': []}, {'inputs': [0], 'function': 'filter_size', 'value_inputs': ['large']}, {'inputs': [1], 'function': 'filter_color', 'value_inputs': ['green']}]\n"
     ]
    }
   ],
   "source": [
    "q = dataset.questions[0]\n",
    "print(type(q[\"program\"]))\n",
    "print(q[\"program\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2d3a450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMN program: [('scene', None), ('attend', 12), ('attend', 5), ('attend', 8), ('attend', 4), ('measure', 'count')]\n",
      "Logits shape: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "modules = {\n",
    "    \"attend\": AttendModule(512, ATTR_VOCAB_SIZE).to(device),\n",
    "    \"relate\": RelateModule(512, REL_VOCAB_SIZE).to(device),\n",
    "    \"combine\": CombineModule().to(device),\n",
    "    \"unique\": UniqueModule(512).to(device),\n",
    "    \"measure\": MeasureModule(NUM_ANSWERS).to(device)\n",
    "}\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "images, programs, answers = batch\n",
    "\n",
    "# choose sample 0 from the batch\n",
    "image = images[0].unsqueeze(0).to(device)\n",
    "program = programs[0]\n",
    "answer = answers[0].to(device)\n",
    "\n",
    "features = backbone(image)\n",
    "\n",
    "converted_program = clevr_to_nmn(program, ATTR_VOCAB)\n",
    "\n",
    "composer = NMNComposer(modules).to(device)\n",
    "logits = composer(features, converted_program)\n",
    "\n",
    "print(\"NMN program:\", converted_program)\n",
    "print(\"Logits shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37657aeb",
   "metadata": {},
   "source": [
    "### Training Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "494f649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, backbone, composer, optimizer, device):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, programs, answers in dataloader:\n",
    "        images = images.to(device)\n",
    "        answers = answers.to(device)\n",
    "\n",
    "        features = backbone(images)\n",
    "\n",
    "        batch_logits = []\n",
    "        for i, prog in enumerate(programs):\n",
    "            converted = clevr_to_nmn(prog, ATTR_VOCAB)\n",
    "            logits = composer(features[i:i+1], converted)\n",
    "            batch_logits.append(logits)\n",
    "\n",
    "        logits = torch.cat(batch_logits, dim=0)\n",
    "        loss = F.cross_entropy(logits, answers)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #for name, p in composer.named_parameters():\n",
    "        #    if p.grad is None:\n",
    "        #        print(\"NO GRAD:\", name)\n",
    "        #    else:\n",
    "        #        print(\"OK:\", name, p.grad.abs().mean().item())\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == answers).sum().item()\n",
    "        total += images.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d83ab50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    composer.parameters(),\n",
    "    lr=3e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0e3ef613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 2.3325 | Accuracy: 0.2760\n",
      "Epoch 2 | Loss: 2.2923 | Accuracy: 0.2770\n",
      "Epoch 3 | Loss: 2.2335 | Accuracy: 0.2820\n",
      "Epoch 4 | Loss: 2.1988 | Accuracy: 0.2860\n",
      "Epoch 5 | Loss: 2.1475 | Accuracy: 0.3100\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss, acc = train_epoch(dataloader, backbone, composer, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
