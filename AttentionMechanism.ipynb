{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ebb85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1\n",
      "Uninstalling torch-2.5.1:\n",
      "  Successfully uninstalled torch-2.5.1\n",
      "Found existing installation: torchvision 0.20.1\n",
      "Uninstalling torchvision-0.20.1:\n",
      "  Successfully uninstalled torchvision-0.20.1\n",
      "Found existing installation: torchaudio 2.5.1\n",
      "Uninstalling torchaudio-2.5.1:\n",
      "  Successfully uninstalled torchaudio-2.5.1\n",
      "Found existing installation: torchtext 0.18.0\n",
      "Uninstalling torchtext-0.18.0:\n",
      "  Successfully uninstalled torchtext-0.18.0\n",
      "Files removed: 1356 (2302.6 MB)\n",
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (25.3)\n",
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp311-none-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.25.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.18.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/nidhimajoju/Library/Python/3.11/lib/python/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/homebrew/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (2.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchtext) (2.32.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchtext) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchtext) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchtext) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchtext) (2026.1.4)\n",
      "Downloading torch-2.10.0-cp311-none-macosx_11_0_arm64.whl (79.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.25.0-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.10.0-cp311-cp311-macosx_11_0_arm64.whl (736 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.4/736.4 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchtext-0.18.0-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision, torchtext, torchaudio\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.13.1\n",
      "\u001b[2K    Uninstalling sympy-1.13.1:\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.1━━━━━\u001b[0m \u001b[32m0/5\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [torchaudio]5\u001b[0m [torchtext]n]\n",
      "\u001b[1A\u001b[2KSuccessfully installed sympy-1.14.0 torch-2.10.0 torchaudio-2.10.0 torchtext-0.18.0 torchvision-0.25.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y torch torchvision torchaudio torchtext\n",
    "!{sys.executable} -m pip cache purge\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464df373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/opt/python@3.11/bin/python3.11\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93280f",
   "metadata": {},
   "source": [
    "Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2f18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a5e48",
   "metadata": {},
   "source": [
    "Dataset Loading & Preprocessing (IMDB)\n",
    "- Tokenization\n",
    "- Vocabulary Construction\n",
    "- Encoding & Padding\n",
    "- DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6650818",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "def basic_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "max_vocab = 20000\n",
    "counter = Counter()\n",
    "for ex in dataset[\"train\"].select(range(20000)):\n",
    "    counter.update(basic_tokenize(ex[\"text\"]))\n",
    "\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "for i, (tok, _) in enumerate(counter.most_common(max_vocab - len(vocab)), start=len(vocab)):\n",
    "    vocab[tok] = i\n",
    "\n",
    "pad_id = vocab[\"<pad>\"]\n",
    "unk_id = vocab[\"<unk>\"]\n",
    "\n",
    "# Encode function to convert text to list of token IDs\n",
    "def encode(text, max_len=256):\n",
    "    toks = basic_tokenize(text)[:max_len]\n",
    "    ids = [vocab.get(t, unk_id) for t in toks]\n",
    "    return ids\n",
    "\n",
    "# Collate function to create batches with padding\n",
    "def collate(batch, max_len=256):\n",
    "    # batch has a list of dicts with text and label\n",
    "    ids_list = [encode(x[\"text\"], max_len=max_len) for x in batch]\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "    lengths = torch.tensor([len(ids) for ids in ids_list], dtype=torch.long)\n",
    "    max_in_batch = max(lengths).item()\n",
    "\n",
    "    x = torch.full((len(ids_list), max_in_batch), pad_id, dtype=torch.long)\n",
    "    for i, ids in enumerate(ids_list):\n",
    "        x[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    return x, lengths, labels\n",
    "\n",
    "train_n = min(25000, len(dataset[\"train\"]))   # IMDB train set has 25k examples\n",
    "test_n  = min(5000,  len(dataset[\"test\"]))\n",
    "\n",
    "# DataLoaders for training and testing\n",
    "train_loader = DataLoader(\n",
    "    dataset[\"train\"].shuffle(seed=0).select(range(train_n)),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate(b, max_len=256)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset[\"test\"].select(range(test_n)),\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate(b, max_len=256)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340f452",
   "metadata": {},
   "source": [
    "Model Architecture\n",
    "- Positional Encoding\n",
    "- Token Embedding\n",
    "- Transformer Encoder Layers\n",
    "- Sequence Pooling\n",
    "- Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ca54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_ff=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=dim_ff, dropout=dropout,\n",
    "            batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T)\n",
    "        mask = (x == pad_id)  # True where pad, for src_key_padding_mask\n",
    "        h = self.emb(x)       # (B, T, D)\n",
    "        h = self.pos(h)\n",
    "        h = self.encoder(h, src_key_padding_mask=mask)  # (B, T, D)\n",
    "\n",
    "        # simple pooling: mean over non-pad tokens\n",
    "        nonpad = (~mask).float().unsqueeze(-1)          # (B, T, 1)\n",
    "        pooled = (h * nonpad).sum(dim=1) / nonpad.sum(dim=1).clamp(min=1.0)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "model = TransformerSentiment(vocab_size=len(vocab)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab91f0d",
   "metadata": {},
   "source": [
    "Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ceed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc=0.727 | test acc=0.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train acc=0.819 | test acc=0.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train acc=0.848 | test acc=0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# instantiate the model \n",
    "model = TransformerSentiment(vocab_size=len(vocab)).to(device)\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "opt = torch.optim.Adam(trainable_params, lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    pbar = tqdm(loader, leave=False)\n",
    "\n",
    "    for xb, lengths, yb in pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        correct += (logits.argmax(-1) == yb).sum().item()\n",
    "        total += bs\n",
    "\n",
    "        pbar.set_postfix(loss=total_loss/total, acc=correct/total)\n",
    "\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    test_loss, test_acc = run_epoch(test_loader, train=False)\n",
    "    print(f\"Epoch {epoch+1}: train acc={train_acc:.3f} | test acc={test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf51c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "/opt/homebrew/lib/python3.11/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__); print(torch.__file__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
