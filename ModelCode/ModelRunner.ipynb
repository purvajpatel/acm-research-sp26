{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5c0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8f8cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144603\n",
      "frame_id    0\n",
      "label       1\n",
      "Name: 0, dtype: int64\n",
      "10796\n",
      "10796\n"
     ]
    }
   ],
   "source": [
    "# first, preprocess data\n",
    "\n",
    "df = pd.read_csv(\"./metadata/training_mixed_frames_labels.csv\", header=0)\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "print(df.iloc[0])\n",
    "\n",
    "class0Data = df[(df[\"label\"] == 0) | (df[\"label\"] == 7)]\n",
    "\n",
    "class1Data = df[(df[\"label\"] == 1) | (df[\"label\"] == 2) | (df[\"label\"] == 3) | (df[\"label\"] == 4) | (df[\"label\"] == 17)]\n",
    "\n",
    "class0Data = class0Data.iloc[::2]\n",
    "class1Data = class1Data.iloc[::2]\n",
    "\n",
    "class0Data[\"label\"] = 0\n",
    "class0Data = class0Data.sample(class1Data.shape[0], random_state=42)\n",
    "\n",
    "class1Data[\"label\"] = 1\n",
    "\n",
    "print(class0Data.shape[0])\n",
    "print(class1Data.shape[0])\n",
    "\n",
    "finalDf = pd.concat([class0Data, class1Data], ignore_index=True)\n",
    "\n",
    "\n",
    "# exclude_indexes = class0Data.index.union(class1Data.index)\n",
    "# df_filtered = df.drop(exclude_indexes)\n",
    "# print(df_filtered.shape)\n",
    "\n",
    "\n",
    "# for frame in range(df_filtered.shape[0]):\n",
    "#     currentImageIndex = df_filtered.iloc[frame][\"frame_id\"]\n",
    "#     imagePath = f\"./unlabeled_set/{currentImageIndex:06d}_512_512.jpg\"\n",
    "#     if os.path.exists(imagePath):\n",
    "#         os.remove(imagePath)\n",
    "#         print(f\"{imagePath} deleted.\")\n",
    "\n",
    "# # list all items in the folder\n",
    "# children = os.listdir(\"./unlabeled_set\")\n",
    "\n",
    "# print(f\"Number of items in: {len(children)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e2a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21592, 96, 96)\n",
      "Made it to index 0!\n",
      "Made it to index 2000!\n",
      "Made it to index 4000!\n",
      "Made it to index 6000!\n",
      "Made it to index 8000!\n",
      "Made it to index 10000!\n",
      "Made it to index 12000!\n",
      "Made it to index 14000!\n",
      "Made it to index 16000!\n",
      "Made it to index 18000!\n",
      "Made it to index 20000!\n"
     ]
    }
   ],
   "source": [
    "images = np.zeros((finalDf.shape[0], 96, 96), dtype=np.float32)\n",
    "print(images.shape)  # (100, 512, 512)\n",
    "\n",
    "def returnProcessedImage(dfIndex, pictureNumber):\n",
    "    currentImage = Image.open(f\"dataset/{pictureNumber:06d}_512_512.jpg\")\n",
    "    # convert this to 96 x 96 and grayscale\n",
    "    processedImage = currentImage.resize([96, 96], Image.Resampling.LANCZOS).convert(\"L\")\n",
    "    normalizedNpArrImage = np.array(processedImage, dtype=np.float32) / 255.0\n",
    "\n",
    "    return (dfIndex, normalizedNpArrImage)\n",
    "\n",
    "# you guys ever just spend a good amount of time  on something only to realize it's all useless? yeah that's what happened right below\n",
    "\n",
    "# currentDfIndex = 0\n",
    "# batchSize = 10\n",
    "# while(currentDfIndex < finalDf.shape[0]):\n",
    "#     with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#             futures = []\n",
    "#             for i in range(0, min(finalDf.shape[0], currentDfIndex + batchSize), 1):\n",
    "#                  dfIndex = currentDfIndex + i\n",
    "#                  futures.append(executor.submit(returnProcessedImage, dfIndex, finalDf.iloc[dfIndex][\"frame_id\"]))\n",
    "#             for future in as_completed(futures):\n",
    "#                  result = future.result()\n",
    "#                  images[result[0]] = result[1]\n",
    "#     currentDfIndex += batchSize\n",
    "#     if(currentDfIndex % 2000 == 0):\n",
    "#         print(f\"Made it to index {currentDfIndex}!\")\n",
    "\n",
    "for i in range(0, finalDf.shape[0], 1):\n",
    "    actualPicIndex = finalDf.iloc[i][\"frame_id\"]\n",
    "    currentImage = Image.open(f\"dataset/{actualPicIndex:06d}_512_512.jpg\")\n",
    "    # convert this to 96 x 96 and grayscale\n",
    "    processedImage = currentImage.resize([96, 96], Image.Resampling.LANCZOS).convert(\"L\")\n",
    "    normalizedNpArrImage = np.array(processedImage, dtype=np.float32) / 255.0\n",
    "\n",
    "    images[i] = normalizedNpArrImage\n",
    "\n",
    "    if(i % 2000 == 0):\n",
    "        print(f\"Made it to index {i}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d0ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = finalDf[\"label\"].to_numpy()\n",
    "\n",
    "# now shuffle them when passing in\n",
    "randomIndexOrder = np.random.permutation(labels.shape[0])\n",
    "\n",
    "images = images[randomIndexOrder]\n",
    "labels = labels[randomIndexOrder]\n",
    "\n",
    "# also must add an extra dimension\n",
    "if images.ndim == 3:  # shape = (num_samples, 96, 96)\n",
    "    images = images[..., np.newaxis]  # add channel dim\n",
    "elif images.ndim != 4:\n",
    "    images = np.squeeze(images, axis=-1)\n",
    "\n",
    "# now make a training and test set\n",
    "testPercentage = 0.85\n",
    "cutoffPoint = int(testPercentage * labels.shape[0])\n",
    "\n",
    "trainingXSet = images[0:cutoffPoint]\n",
    "trainingYSet = labels[0:cutoffPoint]\n",
    "\n",
    "testingXSet = images[cutoffPoint:]\n",
    "testingYSet = labels[cutoffPoint:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2eed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef44122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - AUC: 0.4935 - Precision: 0.4974 - Recall: 0.5534 - accuracy: 0.4956 - loss: 0.6937\n",
      "Epoch 2/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.5327 - Precision: 0.5293 - Recall: 0.5602 - accuracy: 0.5275 - loss: 0.6920\n",
      "Epoch 3/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.6103 - Precision: 0.5751 - Recall: 0.5718 - accuracy: 0.5755 - loss: 0.6735\n",
      "Epoch 4/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.6680 - Precision: 0.6230 - Recall: 0.5881 - accuracy: 0.6213 - loss: 0.6481\n",
      "Epoch 5/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.6672 - Precision: 0.6152 - Recall: 0.5840 - accuracy: 0.6100 - loss: 0.6450\n",
      "Epoch 6/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.6854 - Precision: 0.6325 - Recall: 0.6242 - accuracy: 0.6278 - loss: 0.6329\n",
      "Epoch 7/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.6927 - Precision: 0.6254 - Recall: 0.6283 - accuracy: 0.6250 - loss: 0.6285\n",
      "Epoch 8/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.7081 - Precision: 0.6444 - Recall: 0.6325 - accuracy: 0.6453 - loss: 0.6180\n",
      "Epoch 9/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.7136 - Precision: 0.6438 - Recall: 0.6639 - accuracy: 0.6462 - loss: 0.6145\n",
      "Epoch 10/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.7214 - Precision: 0.6514 - Recall: 0.6628 - accuracy: 0.6553 - loss: 0.6068\n",
      "Epoch 11/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.7344 - Precision: 0.6659 - Recall: 0.6864 - accuracy: 0.6686 - loss: 0.6009\n",
      "Epoch 12/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.7434 - Precision: 0.6764 - Recall: 0.6731 - accuracy: 0.6767 - loss: 0.5924\n",
      "Epoch 13/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.7520 - Precision: 0.6789 - Recall: 0.7082 - accuracy: 0.6846 - loss: 0.5835\n",
      "Epoch 14/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - AUC: 0.7583 - Precision: 0.6857 - Recall: 0.7137 - accuracy: 0.6920 - loss: 0.5776\n",
      "Epoch 15/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.7734 - Precision: 0.7059 - Recall: 0.7258 - accuracy: 0.7068 - loss: 0.5646\n",
      "Epoch 16/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - AUC: 0.7814 - Precision: 0.7062 - Recall: 0.7128 - accuracy: 0.7102 - loss: 0.5573\n",
      "Epoch 17/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.7872 - Precision: 0.7200 - Recall: 0.7465 - accuracy: 0.7261 - loss: 0.5518\n",
      "Epoch 18/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.7945 - Precision: 0.7160 - Recall: 0.7399 - accuracy: 0.7223 - loss: 0.5442\n",
      "Epoch 19/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8019 - Precision: 0.7252 - Recall: 0.7372 - accuracy: 0.7307 - loss: 0.5369\n",
      "Epoch 20/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8076 - Precision: 0.7336 - Recall: 0.7608 - accuracy: 0.7417 - loss: 0.5296\n",
      "Epoch 21/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8057 - Precision: 0.7210 - Recall: 0.7677 - accuracy: 0.7341 - loss: 0.5331\n",
      "Epoch 22/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8189 - Precision: 0.7349 - Recall: 0.7657 - accuracy: 0.7465 - loss: 0.5184\n",
      "Epoch 23/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8237 - Precision: 0.7421 - Recall: 0.7701 - accuracy: 0.7515 - loss: 0.5107\n",
      "Epoch 24/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8406 - Precision: 0.7596 - Recall: 0.7689 - accuracy: 0.7639 - loss: 0.4924\n",
      "Epoch 25/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8371 - Precision: 0.7566 - Recall: 0.7717 - accuracy: 0.7626 - loss: 0.4975\n",
      "Epoch 26/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8505 - Precision: 0.7668 - Recall: 0.7922 - accuracy: 0.7757 - loss: 0.4802\n",
      "Epoch 27/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8580 - Precision: 0.7746 - Recall: 0.7900 - accuracy: 0.7795 - loss: 0.4700\n",
      "Epoch 28/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8660 - Precision: 0.7831 - Recall: 0.8034 - accuracy: 0.7890 - loss: 0.4583\n",
      "Epoch 29/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8748 - Precision: 0.7945 - Recall: 0.8076 - accuracy: 0.8000 - loss: 0.4454\n",
      "Epoch 30/30\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - AUC: 0.8742 - Precision: 0.7856 - Recall: 0.8151 - accuracy: 0.7991 - loss: 0.4447\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # 96 x 96\n",
    "    Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=(96, 96, 1)),\n",
    "    # becomes 48 x 48 after\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    # \n",
    "    Conv2D(32, (2, 2), padding=\"same\", activation='relu'),\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # for binary classification\n",
    "])\n",
    "\n",
    "# tensorflow is werid, so apparently you gotta compile it first? interesting\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'AUC', 'Precision', 'Recall']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    trainingXSet, trainingYSet,\n",
    "    batch_size=16,\n",
    "    epochs=30\n",
    ")\n",
    "\n",
    "predictions = model.predict(testingXSet) \n",
    "\n",
    "# apparently this just works\n",
    "predicted_classes = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b21b3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1295  335]\n",
      " [ 210 1399]]\n",
      "Accuracy:\n",
      " 0.8317381907996295\n",
      "Recall:\n",
      " 0.8694841516469857\n",
      "F1 Score:\n",
      " 0.8369727789410709\n"
     ]
    }
   ],
   "source": [
    "confusionMatrix = confusion_matrix(testingYSet, predicted_classes)\n",
    "print(\"Confusion Matrix:\\n\", confusionMatrix)\n",
    "\n",
    "accuracy = accuracy_score(testingYSet, predicted_classes)\n",
    "print(\"Accuracy:\\n\", accuracy)\n",
    "\n",
    "recall = recall_score(testingYSet, predicted_classes)\n",
    "print(\"Recall:\\n\", recall)\n",
    "\n",
    "f1Score = f1_score(testingYSet, predicted_classes)\n",
    "print(\"F1 Score:\\n\", f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "791553c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# its kinda weird, but it needs a dataset of representations to quantize to see what to scale immediate activation weights as\n",
    "def representative_dataset():\n",
    "    for i in range(400):  # take ~100 samples from your training set\n",
    "        # x_train[i] shape: (96, 96, 1), values between 0 and 1\n",
    "        # Must add batch dimension\n",
    "        yield [trainingXSet[i:i+1].astype('float32')]\n",
    "\n",
    "\n",
    "# begin converting, not only to tflite but also doing quantization\n",
    "\n",
    "# specifies what we're gonna convert to\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# give it the function to calculate middle activations\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Force int8 quantization for both weights and activations\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set input/output to uint8\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aed922d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\mahd\\AppData\\Local\\Temp\\tmphduqi010\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\mahd\\AppData\\Local\\Temp\\tmphduqi010\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\mahd\\AppData\\Local\\Temp\\tmphduqi010'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96, 96, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2145136724880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2145136726032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2145136724688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2145136724304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2145136724112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2145136725456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2145136723920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2145136725648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mahd\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# actually make the quantized small model\n",
    "finalModel = converter.convert()\n",
    "\n",
    "# now, save it to a file\n",
    "with open(\"../cnn_model_uint8.tflite\", \"wb\") as f:\n",
    "    f.write(finalModel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
